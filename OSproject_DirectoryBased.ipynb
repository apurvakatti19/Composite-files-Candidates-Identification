{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict, defaultdict\n",
    "import os\n",
    "import pyfpgrowth;\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "datapath = os.getcwd()\n",
    "log_data_path = datapath + \"/data\" #all log data should kept in this folder\n",
    "result_save_path = datapath + \"/result\" # all generated txt result save here\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_log = log_data_path + \"/Log.txt.strace\"\n",
    "data = pd.read_csv(chrome_log , sep=\" \",header=None,names=[\"Name\",\"Junk1\",\"Junk2\",\"DateTime\",\"Permission\",\"FilePath\",\"Received\",\"BytesAccessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Junk1','Junk2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTimePermission'] = data['DateTime'].str.cat(data['Permission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['DateTime','Permission'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FilePath']=data['FilePath'].str.replace('GET ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTimePermission']=data['DateTimePermission'].str.replace('-0700','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.rename(index=str,columns={\"DateTimePermission\":\"DateTime\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTime'] =  pd.to_datetime(data['DateTime'], format='[%d/%b/%Y:%H:%M:%S]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pass the DF before any preprocessing to generate reply log and uncluster baseline txt\n",
    "\"\"\"\n",
    "def createReplyLog_And_BaseLine(dataFrame):\n",
    "    replay_Log_Name = os.path.join(result_save_path, \"replay_log_user.txt\")\n",
    "    freplay = open(replay_Log_Name,\"w+\")  \n",
    "    for file in dataFrame.FilePath:\n",
    "        freplay.write(file+\"\\n\")\n",
    "    freplay.close()\n",
    "\n",
    "    #Write the cluster.index_base :: all unique filePath as unclustered: As baseline to compare our results\n",
    "    baseCluster_File_Name = os.path.join(result_save_path, \"cluster.index_baseline_user.txt\")    \n",
    "    fbaseCluster = open(baseCluster_File_Name,\"w+\")\n",
    "    fbaseCluster.write(\"Unclustered Files:\\n\")\n",
    "    for file in data.FilePath.unique():\n",
    "        if file==\"\":\n",
    "            continue\n",
    "        fbaseCluster.write(file+\"\\n\")\n",
    "\n",
    "    fbaseCluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reply Log and baseline cluster need to be created only Once, If u need uncomment below and run\n",
    "#createReplyLog_And_BaseLine(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Name','Received'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687405, 3)\n",
      "(560449, 3)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)\n",
    "data=data.drop_duplicates(keep='last') \n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating file length based on '/'\n",
    "datatemp=pd.Series()\n",
    "datatemp=data.assign(FileLength=data.FilePath.str.count('/'))\n",
    "datatemp=datatemp.assign(FileLength=datatemp.FileLength-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DF: 560312\n"
     ]
    }
   ],
   "source": [
    "#pre-processed files without a path;example there were many files which had paths like ''\n",
    "datatemp=datatemp[datatemp.FileLength != -1]\n",
    "datatemp=datatemp[datatemp.FileLength!=0]\n",
    "\n",
    "print (\"Number of rows in DF:\",datatemp.FilePath.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrequencyDict=datatemp.FilePath.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrequencyDict=FrequencyDict.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the frwquency of each file\n",
    "def convertFrequencyDict(columns,FrequencyDict):\n",
    "    frequency=str(columns[0])\n",
    "    try:\n",
    "        Frequency=FrequencyDict[frequency]\n",
    "    except Exception as e:\n",
    "        Frequency=0\n",
    "    return Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatemp['Frequency']=datatemp[['FilePath']].apply(convertFrequencyDict,args=(FrequencyDict,),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping files based on inner most subfolder\n",
    "def groupbysubfolder(columns):\n",
    "    string=str(columns[0])\n",
    "    itemlist=string.split('/')\n",
    "    return itemlist[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatemp['Sub_Folder']=datatemp[['FilePath']].apply(groupbysubfolder,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grouping files based on both same time frame and same inner most folder\n",
    "dataTempGroupBy=datatemp.groupby(['DateTime','Sub_Folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 141035\n"
     ]
    }
   ],
   "source": [
    "dataTempGroupByList=[]\n",
    "for x in dataTempGroupBy.groups:\n",
    "    dataTempGroupByList.append(dataTempGroupBy.get_group(x)['FilePath'].tolist())\n",
    "    \n",
    "print (\"Data Size:\", len(dataTempGroupByList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It generate 2 txt file: \n",
    "    1. Cluster List in txt with your given name of file\n",
    "       cluster.index fileName Convention: cluster.index_algorithm_supportThreshold_timeWindow\n",
    "       \n",
    "    2. All filePath list from dataFrame with yoru given name of file\n",
    "    \n",
    "Args:\n",
    "        patterns_keyList: List of list (each inner list presents a cluster)\n",
    "        DataFrame_filepath: Data Frame file path list\n",
    "        Algo_Name: Algorithm to generate this frequent pattern        \n",
    "        supportThreshold :: used in the algorithm\n",
    "        TimeWindow: which has been used in our transaction preprocessing\n",
    "Optional Arguments: \n",
    "        Feature :: features consider to create transaction, ex : time only, time+directory\n",
    "\"\"\"\n",
    "def createClusterFile(patterns_keyList, DataFrame_filepath, algoName, supportThreshold, TimeWindow, feature =\"\"):\n",
    "\n",
    "    #---------------Results file path-----------------#    \n",
    "    #append the result directory\n",
    "    #DataPath_Name = os.path.join(result_save_path, DataPath_Name+\".txt\")\n",
    "\n",
    "    cluster_File_Name = \"cluster.index_\" + algoName + \"_Support=\" + str(supportThreshold) + \"_timeWindow=\"+ str(TimeWindow) + \"_Features=\"+feature\n",
    "    #append the result directory\n",
    "    cluster_File_Name = os.path.join(result_save_path, cluster_File_Name+\".txt\")              \n",
    "    print (\"Cluster File Name: \", cluster_File_Name)\n",
    "    #--------------------------------------------------#\n",
    "    fCluster = open(cluster_File_Name,\"w+\")    \n",
    "    count = 1;\n",
    "    hash_Dict = {}\n",
    "    for items in patterns_keyList:\n",
    "        inner_keyList = list(items)\n",
    "        if(len(inner_keyList)==1): ## remove single file cluster\n",
    "            continue\n",
    "        fCluster.write(\"Cluster %d\\n\" % (count))\n",
    "        count = count + 1 \n",
    "        for i in inner_keyList:\n",
    "            #print(i)\n",
    "            fCluster.write(\"%s\\n\" %i)\n",
    "            if i not in hash_Dict:\n",
    "                hash_Dict[i] = True    \n",
    "\n",
    "    #create the unclustered filepath\n",
    "    fCluster.write(\"Unclustered Files:\\n\")\n",
    "    #filePath = open(DataPath_Name, \"w+\")\n",
    "\n",
    "    #------------Uncluster Files in cluster.index and all FilePath in DataPath txt-------------#\n",
    "    for i in DataFrame_filepath:\n",
    "      #  filePath.write(\"%s\\n\"%(i.strip(' '))) #all data frame files\n",
    "        if i not in hash_Dict:\n",
    "            fCluster.write(\"%s\\n\"%i) #uncluster files\n",
    "            hash_Dict[i] = True #Removing duplicates in uncluster list\n",
    "\n",
    "    fCluster.close()  \n",
    "    #filePath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------closed Itemset calculation from frequent itemset------------\n",
    "def cal_closed_itemset(itemsetList):\n",
    "\n",
    "    maximal_itemset = []\n",
    "    for index_i in range (0,len(itemsetList)): \n",
    "        cluster_i_set = set(itemsetList[index_i])    \n",
    "        flag = True\n",
    "        for index_j in range (index_i+1, len(itemsetList)):\n",
    "            cluster_j_set = set(itemsetList[index_j])    \n",
    "            if(cluster_i_set.issubset(cluster_j_set)):\n",
    "                #print (cluster_i_set , \"<=>\", cluster_j_set)\n",
    "                flag = False\n",
    "                break\n",
    "        if(flag):\n",
    "            maximal_itemset.append(itemsetList[index_i])\n",
    "    #print (\"Maximal Itemset:\", maximal_itemset)\n",
    "    print (\"size of maximal Itemsets\", len(maximal_itemset))\n",
    "    return maximal_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Size of All frequest Itemset:  2671\n",
      "size of maximal Itemsets 814\n",
      "Cluster File Name:  /Users/Mehedi/BitBucket/AdvancedOS Project/result/cluster.index_FPGrowth_Support=100_timeWindow=1_Features= time+Directory.txt\n"
     ]
    }
   ],
   "source": [
    "#Try FP_Growth algorithm on this transaction set\n",
    "#print (time_transactions)\n",
    "\n",
    "#Use find_frequent_patterns to find patterns in baskets that occur over the support threshold:\n",
    "minSupport = 100\n",
    "patterns = pyfpgrowth.find_frequent_patterns(dataTempGroupByList, minSupport)  # need to generate data using this param\n",
    "print (\"pattern:\")\n",
    "keyList = []\n",
    "for key,val in patterns.items():\n",
    "    keyList.append(key)\n",
    "\n",
    "print (\"\\n\\n\\n\")\n",
    "#print (\"All frequent items: \", keyList)\n",
    "print (\"Size of All frequest Itemset: \", len(keyList))\n",
    "closed_itemset = cal_closed_itemset(keyList)\n",
    "\n",
    "\n",
    "timed_window = 1 #since it groupby with time and directory\n",
    "feature = \" time+Directory\"\n",
    "createClusterFile(closed_itemset, datatemp.FilePath, \"FPGrowth\",minSupport,timed_window,feature)\n",
    "\n",
    "\n",
    "\n",
    "#Use generate_association_rules to find patterns that are associated with another with a certain minimum probability:\n",
    "#rules = pyfpgrowth.generate_association_rules(patterns, 0.5)  # generate data using this prob \n",
    "\n",
    "#print(\"\\nAssociation Rules :\")\n",
    "#for key,val in rules.items():\n",
    "#    print (key , \"=>\" , val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions counts:  141035\n",
      "Apriori itemset size 0\n",
      "\n",
      "\n",
      "Apriori List:  0\n",
      "Size of All frequest Itemset:  0\n",
      "size of maximal Itemsets 0\n",
      "Cluster File Name:  /Users/Mehedi/BitBucket/AdvancedOS Project/result/cluster.index_Apriori_cluster_Support=0.1_timeWindow=1_Features=time+Directory.txt\n"
     ]
    }
   ],
   "source": [
    "##Try Apriori Algorithm with features minSup, minCon,\n",
    "print (\"Transactions counts: \",len(dataTempGroupByList))\n",
    "\n",
    "minSupport = 0.10\n",
    "itemsets, rules = apriori(dataTempGroupByList, minSupport)\n",
    "print (\"Apriori itemset size\", len(itemsets))\n",
    "\n",
    "#Formating the itemsets \n",
    "itemsetList = []\n",
    "for key,val in itemsets.items():\n",
    "    temp = ()\n",
    "    for i in val:\n",
    "        temp+=(i) #tuple concatenation\n",
    "    itemsetList.append(tuple(set(temp)))  # remove duplicate from each cluster\n",
    "print (\"\\n\\nApriori List: \", len(itemsetList))\n",
    "#--------------------------------//  \n",
    "\n",
    "#print (\"All frequent items: \", itemsetList)\n",
    "print (\"Size of All frequest Itemset: \", len(itemsetList))\n",
    "closed_itemset = cal_closed_itemset(itemsetList)\n",
    "\n",
    "timed_window=1 #since it groupby with time and directory\n",
    "feature = \"time+Directory\"\n",
    "createClusterFile(closed_itemset, datatemp.FilePath,\"Apriori_cluster\",minSupport, timed_window,feature)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
