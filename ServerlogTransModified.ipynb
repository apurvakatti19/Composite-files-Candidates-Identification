{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict, defaultdict\n",
    "import os\n",
    "import pyfpgrowth;\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "datapath = os.getcwd()\n",
    "log_data_path = datapath + \"/data\" #all log data should kept in this folder\n",
    "result_save_path = datapath + \"/result\" # all generated txt result save here\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "chrome_log = log_data_path + \"/Log.txt.access_log.2015\"\n",
    "data = pd.read_csv(chrome_log , sep=\" \",header=None,names=[\"IP\",\"Junk1\",\"Junk2\",\"DateTime\",\"Permission\",\"FilePath\",\"Status\",\"BytesAccessed\",\"Junk3\",\"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Junk1','Junk2','Junk3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTimePermission'] = data['DateTime'].str.cat(data['Permission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['DateTime','Permission'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTTP request info from the filepaths\n",
    "data['FilePath']=data['FilePath'].str.replace('GET ','')\n",
    "data['FilePath']=data['FilePath'].str.replace('POST ','')\n",
    "data['FilePath']=data['FilePath'].str.replace('HEAD ','')\n",
    "data['FilePath']=data['FilePath'].str.replace('OPTIONS ','')\n",
    "data['FilePath']=data['FilePath'].str.replace('PROPFIND ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FilePath']=data['FilePath'].str.replace(' HTTP/1.1','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTimePermission']=data['DateTimePermission'].str.replace('-0500','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTimePermission']=data['DateTimePermission'].str.replace('-0400','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.rename(index=str,columns={\"DateTimePermission\":\"DateTime\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Status'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.sort_values(by=['IP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pass the DF before any preprocessing to generate reply log and uncluster baseline txt\n",
    "\"\"\"\n",
    "def createReplyLog_And_BaseLine(dataFrame):\n",
    "    replay_Log_Name = os.path.join(result_save_path, \"replay_log_user.txt\")\n",
    "    freplay = open(replay_Log_Name,\"w+\")  \n",
    "    for file in dataFrame.FilePath:\n",
    "        freplay.write(file+\"\\n\")\n",
    "    freplay.close()\n",
    "\n",
    "    #Write the cluster.index_base :: all unique filePath as unclustered: As baseline to compare our results\n",
    "    baseCluster_File_Name = os.path.join(result_save_path, \"cluster.index_baseline_user.txt\")    \n",
    "    fbaseCluster = open(baseCluster_File_Name,\"w+\")\n",
    "    fbaseCluster.write(\"Unclustered Files:\\n\")\n",
    "    for file in data.FilePath.unique():\n",
    "        if file==\"\":\n",
    "            continue\n",
    "        fbaseCluster.write(file+\"\\n\")\n",
    "\n",
    "    fbaseCluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reply Log and baseline cluster need to be created only Once, If u need uncomment below and run\n",
    "#createReplyLog_And_BaseLine(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before this preprocessing data size:  1000000\n",
      "After . preprocessing data size:  721282\n"
     ]
    }
   ],
   "source": [
    "#Ritu's changes starts from here .....\n",
    "print (\"Before this preprocessing data size: \", data.FilePath.size)\n",
    "data_filtered = (data.set_index('FilePath').filter(like='/~', axis=0).reset_index())[data.columns]\n",
    "print (\"After . preprocessing data size: \",data_filtered.FilePath.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered['DateTime'] = data_filtered['DateTime'].str.strip('[')\n",
    "data_filtered['DateTime'] = data_filtered['DateTime'].str.strip(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe according to date\n",
    "data_list = data_filtered['DateTime'].str.split(':')\n",
    "dateList = []\n",
    "for items in data_list:\n",
    "    dateList.append(items[0])\n",
    "\n",
    "data_filtered['SortedDateTime'] =pd.to_datetime(dateList)\n",
    "data_filtered = data_filtered.sort_values(by=['SortedDateTime'], ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n"
     ]
    }
   ],
   "source": [
    "#Get unique datelist from the SortedDateTime\n",
    "uniqueDateList = []\n",
    "uniqueDateList = data_filtered.SortedDateTime.unique()\n",
    "print(len(uniqueDateList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dates\n",
    "uniqueDateListPart1 = uniqueDateList[:495]\n",
    "uniqueDateListPart2 = uniqueDateList[495:497]\n",
    "uniqueDateListPart3 = uniqueDateList[497:498]\n",
    "uniqueDateListPart4 = uniqueDateList[498:499]\n",
    "uniqueDateListPart5 = uniqueDateList[499:500]\n",
    "uniqueDateListPart6 = uniqueDateList[500:502]\n",
    "uniqueDateListPart7 = uniqueDateList[502:504]\n",
    "uniqueDateListPart8 = uniqueDateList[504:505]\n",
    "uniqueDateListPart9 = uniqueDateList[505:506]\n",
    "uniqueDateListPart10 = uniqueDateList[506:507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame Shapes:\n",
      "(49799, 6)\n",
      "(90816, 6)\n",
      "(39067, 6)\n",
      "(47969, 6)\n",
      "(47957, 6)\n",
      "(105218, 6)\n",
      "(161629, 6)\n",
      "(46775, 6)\n",
      "(60449, 6)\n",
      "(71603, 6)\n"
     ]
    }
   ],
   "source": [
    "#DataFrames constructed by splitting dates\n",
    "dataFramePart1 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart1)]\n",
    "dataFramePart2 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart2)]\n",
    "dataFramePart3 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart3)]\n",
    "dataFramePart4 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart4)]\n",
    "dataFramePart5 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart5)]\n",
    "dataFramePart6 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart6)]\n",
    "dataFramePart7 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart7)]\n",
    "dataFramePart8 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart8)]\n",
    "dataFramePart9 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart9)]\n",
    "dataFramePart10 = data_filtered.loc[data_filtered.SortedDateTime.isin(uniqueDateListPart10)]\n",
    "print (\"Data Frame Shapes:\")\n",
    "print (dataFramePart1.shape)\n",
    "print (dataFramePart2.shape)\n",
    "print (dataFramePart3.shape)\n",
    "print (dataFramePart4.shape)\n",
    "print (dataFramePart5.shape)\n",
    "print (dataFramePart6.shape)#2nd largest (We will avoid this two DF)\n",
    "print (dataFramePart7.shape)#largest\n",
    "print (dataFramePart8.shape)\n",
    "print (dataFramePart9.shape)\n",
    "print (dataFramePart10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing reset of indexes\n",
    "#dataFramePart2 = dataFramePart2.reset_index(drop=True)\n",
    "#dataFramePart2.FilePath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It generate 2 txt file: \n",
    "    1. Cluster List in txt with your given name of file\n",
    "       cluster.index fileName Convention: cluster.index_algorithm_supportThreshold_timeWindow\n",
    "       \n",
    "    2. All filePath list from dataFrame with yoru given name of file\n",
    "    \n",
    "Args:\n",
    "        patterns_keyList: List of list (each inner list presents a cluster)\n",
    "        DataFrame_filepath: Data Frame file path list\n",
    "        Algo_Name: Algorithm to generate this frequent pattern        \n",
    "        supportThreshold :: used in the algorithm\n",
    "        TimeWindow: which has been used in our transaction preprocessing\n",
    "Optional Arguments: \n",
    "        Feature :: features consider to create transaction, ex : time only, time+directory\n",
    "\"\"\"\n",
    "def createClusterFile(patterns_keyList, DataFrame_filepath, algoName, supportThreshold, TimeWindow, feature =\"\"):\n",
    "\n",
    "    #---------------Results file path-----------------#    \n",
    "    #append the result directory\n",
    "    #DataPath_Name = os.path.join(result_save_path, DataPath_Name+\".txt\")\n",
    "\n",
    "    cluster_File_Name = \"cluster.index_\" + algoName + \"_Support=\" + str(supportThreshold) + \"_timeWindow=\"+ str(TimeWindow) + \"_Features=\"+feature\n",
    "    #append the result directory\n",
    "    cluster_File_Name = os.path.join(result_save_path, cluster_File_Name+\".txt\")              \n",
    "    print (\"Cluster File Name: \", cluster_File_Name)\n",
    "    #--------------------------------------------------#\n",
    "    fCluster = open(cluster_File_Name,\"w+\")    \n",
    "    count = 1;\n",
    "    hash_Dict = {}\n",
    "    for items in patterns_keyList:\n",
    "        inner_keyList = list(items)\n",
    "        if(len(inner_keyList)==1): ## remove single file cluster\n",
    "            continue\n",
    "        fCluster.write(\"Cluster %d\\n\" % (count))\n",
    "        count = count + 1 \n",
    "        for i in inner_keyList:\n",
    "            #print(i)\n",
    "            fCluster.write(\"%s\\n\" %i)\n",
    "            if i not in hash_Dict:\n",
    "                hash_Dict[i] = True    \n",
    "\n",
    "    #create the unclustered filepath\n",
    "    fCluster.write(\"Unclustered Files:\\n\")\n",
    "    #filePath = open(DataPath_Name, \"w+\")\n",
    "\n",
    "    #------------Uncluster Files in cluster.index and all FilePath in DataPath txt-------------#\n",
    "    for i in DataFrame_filepath:\n",
    "      #  filePath.write(\"%s\\n\"%(i.strip(' '))) #all data frame files\n",
    "        if i not in hash_Dict:\n",
    "            fCluster.write(\"%s\\n\"%i) #uncluster files\n",
    "            hash_Dict[i] = True #Removing duplicates in uncluster list\n",
    "\n",
    "    fCluster.close()  \n",
    "    #filePath.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the dataframe to include the ones you want time_transactions for\n",
    "\n",
    "#Only change DF here to test with different DF\n",
    "dataFramevar = dataFramePart1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "Total Data Size:  49799\n",
      "No of Time Based Transactions: \n",
      " 7326\n"
     ]
    }
   ],
   "source": [
    "#Set a time window to prepare transactions:: timed_window no of different timed accessed files will be on same transaction.\n",
    "\n",
    "#Number_of_transactions\n",
    "#All secone based transaction will be in this list\n",
    "time_transactions = []\n",
    "time_transactions.clear()\n",
    "\n",
    "#temp-> to be pushed into time_transactions\n",
    "#tempTransactions = []\n",
    "tempTransactions = set()\n",
    "tempTransactions.clear()\n",
    "print (type(tempTransactions))\n",
    "\n",
    "previous_date_time = dataFramevar.DateTime[0] # just to avoid error\n",
    "print (\"Total Data Size: \", dataFramevar.FilePath.size)  \n",
    "\n",
    "#sample data \n",
    "#print (data_filtered.FilePath[0])\n",
    "\n",
    "timed_window = 5;  # Time Window -- Can set the number of different time window we want in one transaction\n",
    "count_window = 0; #ref counting\n",
    "\n",
    "for i in range (0,dataFramevar.FilePath.size-1):\n",
    "    try:\n",
    "\n",
    "        #if time-stamp is different then push the temporary list into the actual transaction\n",
    "        if(previous_date_time != dataFramevar.DateTime[i]):\n",
    "            count_window = count_window + 1\n",
    "            #print (\"Debug Log:\",count_window, \" \", data_filtered.DateTime[i]) \n",
    "            if(count_window>=timed_window):\n",
    "             #   print (\"\\n\\n\")\n",
    "                count_window = 0\n",
    "              #  print (\"Debug Log:\",count_window, \" \", data_filtered.DateTime[i]) \n",
    "                tempTransactionsList = list(tempTransactions)\n",
    "                tempTransactionsList = [x.strip(' ') for x in tempTransactionsList]\n",
    "                time_transactions.append(tempTransactionsList)\n",
    "                # Need to convert before pushed into transaction\n",
    "                tempTransactions.clear()  \n",
    "                \n",
    "            \n",
    "            \n",
    "        tempTransactions.add(dataFramevar.FilePath[i])                           \n",
    "        previous_date_time = dataFramevar.DateTime[i] #update previous timestamp\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "\n",
    "\n",
    "print (\"No of Time Based Transactions: \\n\", len(time_transactions))\n",
    "\n",
    "#print (time_transactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of cluster:  353\n",
      "Cluster File Name:  /Users/Mehedi/BitBucket/AdvancedOS Project/result/cluster.index_FPGrowth_Support=100_timeWindow=5_Features=ServerLog1time.txt\n"
     ]
    }
   ],
   "source": [
    "#Try FP_Growth algorithm on this transaction set\n",
    "#print (time_transactions)\n",
    "\n",
    "#Use find_frequent_patterns to find patterns in baskets that occur over the support threshold:\n",
    "minSupport = 100\n",
    "patterns = pyfpgrowth.find_frequent_patterns(time_transactions, minSupport)  # need to generate data using this param\n",
    "print (\"pattern:\")\n",
    "\n",
    "itemsetList = []\n",
    "for key,val in patterns.items():\n",
    "    itemsetList.append(key)\n",
    "    \n",
    "print (\"\\n\\n\\n\")\n",
    "print (\"Number of cluster: \",len(itemsetList))\n",
    "feature = \"ServerLog1time\"\n",
    "createClusterFile(itemsetList, dataFramevar.FilePath, \"FPGrowth\",minSupport,timed_window,feature)\n",
    "#Use generate_association_rules to find patterns that are associated with another with a certain minimum probability:\n",
    "#rules = pyfpgrowth.generate_association_rules(patterns, 0.5)  # generate data using this prob \n",
    "\n",
    "#print(\"\\nAssociation Rules :\")\n",
    "#for key,val in rules.items():\n",
    "#    print (key , \"=>\" , val)\n",
    "\n",
    "\n",
    "#Ritu's changes ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apurva's changes starts here ...\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '3100[11/Jan/2015:11:36:02' does not match format '[%d/%b/%Y:%H:%M:%S]' (match)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4755d48dbcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DateTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DateTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'[%d/%b/%Y:%H:%M:%S]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         result = array_strptime(arg, format, exact=exact,\n\u001b[0;32m--> 347\u001b[0;31m                                                 errors=errors)\n\u001b[0m\u001b[1;32m    348\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mtslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '3100[11/Jan/2015:11:36:02' does not match format '[%d/%b/%Y:%H:%M:%S]' (match)"
     ]
    }
   ],
   "source": [
    "data['DateTime']=pd.to_datetime(data['DateTime'],format='[%d/%b/%Y:%H:%M:%S]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EpochTime']=pd.DataFrame((data['DateTime'] - dt.datetime(1970,1,1)).dt.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagroupby=data.groupby([pd.cut(data['EpochTime'],\n",
    "                    np.arange(data['EpochTime'].min(),\n",
    "                             data['EpochTime'].max(),900)),'IP','UserAgent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTempGroupByList=[]\n",
    "count=0\n",
    "countsessions=0\n",
    "for x in datagroupby.groups:\n",
    "    try:\n",
    "        datagroupby.get_group(x)\n",
    "        dataTempGroupByList.append(datagroupby.get_group(x))\n",
    "        countsessions=countsessions+1\n",
    "    except KeyError:\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTempGroupByList[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsessions\n",
    "#Apurva's changes ends here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list = data_filtered['FilePath'].str.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userList = []\n",
    "#for items in data_list:\n",
    "#    userList.append(items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userIdDF = pd.DataFrame({'userId': userList})\n",
    "#data_filtered = data_filtered.join(userIdDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check no of files accessed by a particular user\n",
    "#data_filtered.loc[data_filtered['userId'] == '~duan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove favicon.ico files as these doesn't represent a web page\n",
    "#data_filtered = data_filtered[data_filtered.FilePath != '/favicon.ico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tested with just 30000 data\n",
    "#data_random_sample = data_filtered.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataGroupByUserId=data_random_sample.groupby(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataGroupByList=[]\n",
    "#for x in dataGroupByUserId.groups:\n",
    "#    dataGroupByList.append(dataGroupByUserId.get_group(x)['FilePath'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataGroupByList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
